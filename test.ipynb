{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"Evaluating\")? (<ipython-input-1-985f38540639>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-985f38540639>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    print \"Evaluating\"\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"Evaluating\")?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from sklearn.metrics import mean_absolute_error, explained_variance_score,mean_squared_error\n",
    "\n",
    "def run_eval(Y_pred, Y_true, l, pbFlag,idOar=\"\",printError=False):\n",
    "    print(\"Evaluating\")\n",
    "\n",
    "    if (pbFlag == 'landmark'):\n",
    "        # We need to change the shape of the Y_pred and Y_true matrices because the evaluation is different than in Biwi\n",
    "        print Y_pred.shape\n",
    "        Y_pred2 = np.reshape(Y_pred, (5*Y_pred.shape[0],2), order='C')\n",
    "        Y_true2 = np.reshape(Y_true, (5*Y_true.shape[0],2), order='C')\n",
    "        \n",
    "        # mean squared error\n",
    "        err = np.sqrt(np.sum((Y_pred2-Y_true2)**2, axis=1))\n",
    "\n",
    "        listErr = np.empty((5,1))\n",
    "        listFailures = np.empty((5,1))\n",
    "        for i in range(5):\n",
    "            temp = 0\n",
    "            tempFailures = 0\n",
    "            for j in xrange(i,len(err),5):\n",
    "                temp += (err[j]/float(l))\n",
    "                # If an error is larger than 5%, it is counted as failure.\n",
    "                if (err[j]/float(l)) > 0.05:\n",
    "                    tempFailures += 1\n",
    "            listErr[i,0] = temp/(float(len(err))/5)\n",
    "            listFailures[i,0] = tempFailures/(float(len(err))/5)\n",
    "\n",
    "        print('Avg Detection Error:', listErr)\n",
    "        print('Failure Rate:',listFailures)\n",
    "        for x in listFailures:\n",
    "            print \" $\" + str(100*x[0])+ \"$ &\"\n",
    "        \" \".join([str(100*x[0]) for x in listFailures])\n",
    "        print np.mean(np.asarray([x[0] for x in listFailures]))\n",
    "        \n",
    "    elif (pbFlag == 'FBP'):\n",
    "        LOGpred=\"/services/scratch/perception/slathuil/log/error_Training_\"+str(idOar)+pbFlag+\".pickle\"\n",
    "        pickle.dump((Y_pred,Y_true),open(LOGpred,\"w+\"))\n",
    "\n",
    "        listSegments=[(0,1),(1,2),(3,4),(4,5),(6,7),(7,8),(9,10),(10,11),(12,13)]\n",
    "        errX=np.empty((Y_pred.shape[0],14))\n",
    "        errY=np.empty((Y_pred.shape[0],14))\n",
    "        err=np.empty((Y_pred.shape[0],14))\n",
    "        for j in range(14):\n",
    "            errX[:,j]=Y_pred[:,2*j]-Y_true[:,2*j]\n",
    "            errY[:,j]=Y_pred[:,2*j+1]-Y_true[:,2*j+1]\n",
    "\n",
    "        # compute the error for the point in the middle of 3 and 4\n",
    "        XYTorso_true=np.empty((Y_pred.shape[0],2))\n",
    "        XYTorso_pred=np.empty((Y_pred.shape[0],2))\n",
    "        XYTorso_true[:,0]=0.5*(Y_true[:,2*2]+Y_true[:,2*3])\n",
    "        XYTorso_true[:,1]=0.5*(Y_true[:,2*2+1]+Y_true[:,2*3+1])\n",
    "        XYTorso_pred[:,0]=0.5*(Y_pred[:,2*2]+Y_pred[:,2*3])\n",
    "        XYTorso_pred[:,1]=0.5*(Y_pred[:,2*2+1]+Y_pred[:,2*3+1])\n",
    "        errTorso=np.sqrt((XYTorso_pred[:,0]-XYTorso_true[:,0])**2+(XYTorso_pred[:,1]-XYTorso_true[:,1])**2,)\n",
    "\n",
    "\n",
    "        \n",
    "        err = np.sqrt((errX)**2+(errY)**2)\n",
    "        \n",
    "        lengthSegm=np.empty((Y_pred.shape[0],len(listSegments)))\n",
    "        lengthTorso=np.empty(Y_pred.shape[0])\n",
    "\n",
    "        for idSegm,seg in enumerate(listSegments):\n",
    "            lengthSegm[:,idSegm]=np.sqrt((Y_true[:,seg[0]]-Y_true[:,seg[1]])**2)\n",
    "\n",
    "        lengthTorso[:]=np.sqrt((XYTorso_true[:,0]-Y_true[:,2*12])**2+(XYTorso_true[:,1]-Y_true[:,2*12+1])**2)\n",
    "            \n",
    "        correct=np.empty((Y_pred.shape[0],len(listSegments)))\n",
    "        correctTorso=np.empty(Y_pred.shape[0])\n",
    "        for i in range(Y_pred.shape[0]):\n",
    "            for idSegm,seg in enumerate(listSegments):\n",
    "                if (err[i,seg[0]]/lengthSegm[i,idSegm])<0.5 and (err[i,seg[1]]/lengthSegm[i,idSegm])<0.5:\n",
    "                    correct[i,idSegm]=1.0\n",
    "                else:\n",
    "                    correct[i,idSegm]=0.0\n",
    "            if errTorso[i]/lengthTorso[i]<0.5 and err[i,12]/lengthTorso[i]<0.5:\n",
    "                correctTorso[i]=1.0\n",
    "            else:\n",
    "                correctTorso[i]=0.0\n",
    "                \n",
    "        PCP=np.sum(correct,axis=0)/Y_pred.shape[0]\n",
    "        PCPTorso=np.sum(correctTorso)/Y_pred.shape[0]\n",
    "        print \"head:   \" + str(PCP[8])\n",
    "        print \"Torso:  \" + str(PCPTorso)\n",
    "        print \"U Legs: \" + str((PCP[1]+PCP[2])/2.0)\n",
    "        print \"L Legs: \" + str((PCP[0]+PCP[3])/2.0)\n",
    "        print \"U Arms: \" + str((PCP[5]+PCP[6])/2.0)\n",
    "        print \"L Arms: \" + str((PCP[4]+PCP[7])/2.0)\n",
    "        print \"FB:     \" + str((np.sum(PCP)+PCPTorso)/10.0)\n",
    "\n",
    "    \n",
    "    # mean absolute error\n",
    "    MSE = mean_squared_error(Y_true, Y_pred, multioutput='raw_values')\n",
    "    MAE = mean_absolute_error(Y_true, Y_pred, multioutput='raw_values')\n",
    "    evs = explained_variance_score(Y_true, Y_pred, multioutput='raw_values')\n",
    "\n",
    "    # Head pose estimation: pitch, yaw, roll\n",
    "    print('Mean square error:', MSE,np.sum(MSE)/MSE.shape[0])\n",
    "    print('Mean absolute error:', MAE,np.sum(MAE)/MAE.shape[0])\n",
    "    print('Explained variances score:', evs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
